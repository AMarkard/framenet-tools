import torch
import torch.nn as nn
import torchvision.datasets as dsets
import torchvision.transforms as transforms
from torch.autograd import Variable
import torchtext
from torchtext import data
from torchtext import vocab

from framenet_tools.frame_identification.reader import Data_reader
from framenet_tools.frame_identification.frame_id_network import Frame_id_network

use_cuda = True
batch_size = 1

#The files generated by pyfn
train_file = ["../data/experiments/xp_001/data/train.sentences", "../data/experiments/xp_001/data/train.frame.elements"]
dev_file = ["../data/experiments/xp_001/data/dev.sentences", "../data/experiments/xp_001/data/dev.frames"]
test_file = ["../data/experiments/xp_001/data/test.sentences", "../data/experiments/xp_001/data/test.frames"]


class Frame_Identifier(object):

    def get_dataset(self,file):
        ''' Loads the dataset and combines the necessary data 

            Args:
                file: A list of the two files to load

            Returns:
                xs: A list of senctences
                ys: A list of frames corresponding to the given sentences appended with its FEE

        '''
        reader = Data_reader(file[0], file[1])
        reader.read_data()
        dataset = reader.get_dataset()

        xs = [[i[4]]+i[0] for i in dataset]
        
        ys = [i[1] for i in dataset]

        return xs, ys

    def prepare_dataset(self, xs, ys):
        ''' Prepares the dataset and returns a BucketIterator of the dataset

            Args:
                xs: A list of senctences
                ys: A list of frames corresponding to the given sentences

            Returns:
                A BucketIterator of the dataset

        '''
        examples = [data.Example.fromlist([x,y], self.data_fields) for x,y in zip(xs,ys)]

        dataset = data.Dataset(examples, fields=self.data_fields)

        self.input_field.build_vocab(dataset)
        self.output_field.build_vocab(dataset)

        iterator = data.BucketIterator(dataset, batch_size=batch_size, shuffle=False)

        return iterator

    def main(self):

        xs, ys = self.get_dataset(train_file)

        dev_xs, dev_ys = self.get_dataset(dev_file)

        dataset_size = len(xs)

        #Create fields
        self.input_field = data.Field(dtype=torch.float, use_vocab=True, preprocessing=None)#, fix_length= max_length) #No padding necessary anymore, since avg
        self.output_field = data.Field(dtype=torch.long)
        self.data_fields = [('Sentence', self.input_field), ('Frame', self.output_field)]

       
        train_iter = self.prepare_dataset(xs, ys)
        dev_iter = self.prepare_dataset(dev_xs, dev_ys)


        self.input_field.vocab.load_vectors("glove.6B.300d")


        input_count = len(self.input_field.vocab)
        num_classes = len(self.output_field.vocab)
        

        embed = nn.Embedding.from_pretrained(self.input_field.vocab.vectors)

        network = Frame_id_network(True, embed, num_classes)

        network.train_model(train_iter, dataset_size, batch_size)
        acc =network.eval_model(dev_iter)

        print(acc)



f_i = Frame_Identifier()
f_i.main()
